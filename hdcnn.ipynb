{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchically Deep Convolutional Neural Network For Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "import time\n",
    "import os\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "if not os.path.exists('./data/models/'):\n",
    "    os.mkdir('./data/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "File_Exists_Error",
     "evalue": "preped_data_cifar.npz already exists,are you sure you want to overwrite it?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFile_Exists_Error\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e8d4cfe750b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_c_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_c_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfine2coarse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbig_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fine2coarse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFile_Exists_Error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preped_data_cifar.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFile_Exists_Error\u001b[0m: preped_data_cifar.npz already exists,are you sure you want to overwrite it?"
     ]
    }
   ],
   "source": [
    "if any([True if 'preped_data_cifar' in file_name else False for file_name in os.listdir('./data/')]):\n",
    "    with np.load(file='./data/preped_data_cifar.npz') as big_load:\n",
    "        X = big_load['train_arr']\n",
    "        x_test = big_load['test_arr']\n",
    "        y = big_load['y_fine']\n",
    "        y_test = big_load['y_fine_test'] \n",
    "        y_c = big_load['y_c_train']\n",
    "        y_c_test = big_load['y_c_test']\n",
    "        fine2coarse=big_load['fine2coarse']\n",
    "    raise File_Exists_Error('preped_data_cifar.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of coarse categories, max is 20\n",
    "coarse_categories = [1, 2]\n",
    "batch_size, stop, course_history = 32, 4, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little bit about the CIFAR100 Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('./cifar100.txt', header=0, sep='\\t', index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Cifar100 Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y_c), (x_test, y_c_test) = cifar100.load_data(label_mode='coarse')\n",
    "(_, y), (_, y_test) = cifar100.load_data(label_mode='fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_class, first_inds = np.unique(y_c, return_index=True)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=10, squeeze=True, figsize=(20, 5))\n",
    "for row in range(axes.shape[0]):\n",
    "    for ind, ax in enumerate(axes[row, :]):\n",
    "        ind = ind + (axes.shape[1] * row)\n",
    "        ax.imshow(X[first_inds[ind]])\n",
    "        ax.set_title('Coarse: {0}\\n fine: {1}'.format(super_class[ind], y[first_inds[ind]]))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_c, y, X), (y_c_test, y_test, x_test) = \\\n",
    "    tuple([tuple([elm[np.isin(mem[0], coarse_categories)[:,0]] for elm in mem]) \\\n",
    "        for mem in [(y_c, y, X), (y_c_test, y_test, x_test)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_class, first_inds = np.unique(y, return_index=True)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, squeeze=True, figsize=(20, 5))\n",
    "for row in range(axes.shape[0]):\n",
    "    for ind, ax in enumerate(axes[row, :]):\n",
    "        ind = ind + (axes.shape[1] * row)\n",
    "        ax.imshow(X[first_inds[ind]])\n",
    "        ax.set_title('fine: {0}'.format(fine_class[ind]))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-To-Coarse Mapping**\n",
    "\n",
    "(Ideally, this would be done through spectral clustering as opposed to hard-coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine2coarse = np.zeros((len(coarse_categories), 5))\n",
    "for i in coarse_categories:\n",
    "    index = np.where(y_c[:,0] == i)[0]\n",
    "    fine_cat = np.unique([y[j,0] for j in index])\n",
    "    fine2coarse[i-1] = fine_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kr.utils.to_categorical(np.where(fine2coarse.flatten()==y)[1])\n",
    "y_test = kr.utils.to_categorical(np.where(fine2coarse.flatten()==y_test)[1])\n",
    "y_c = kr.utils.to_categorical(np.where(coarse_categories==y_c)[1])\n",
    "y_c_test = kr.utils.to_categorical(np.where(coarse_categories==y_c_test)[1])\n",
    "print(np.shape(y_c), np.shape(y_c_test), np.shape(y), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply ZCA Whitening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "X,x_test = zca(X,x_test)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - ZCA Whitening: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resize Images to be compatible with Xception**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "X = resize(X,10)\n",
    "x_test = resize(x_test)\n",
    "time2 = time.time()\n",
    "print('Time Elapsed - Resizing: '+str(time2-time1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, x_test.shape, y_test.shape, y_c.shape, y_c_test.shape, fine2coarse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(file='./data/preped_data_cifar.npz', train_arr=X, test_arr=x_test, y_fine=y, y_fine_test=y_test,\n",
    "                   y_c_train=y_c, y_c_test=y_c_test, fine2coarse=fine2coarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Training set into Training and Validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "_, _, y_c_train, y_c_val = train_test_split(X, y_c, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Course Labels into train and validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Xception Pretrained on Imagenet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any([True if 'xception_coarse' in file_name else False for file_name in os.listdir('./data/models/')]):\n",
    "    raise File_Exists_Error('xception_coarse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation credit for Xception model to:\n",
    "\n",
    "Chollet Francois. “Xception: Deep Learning with Depthwise Separable Convolutions.” 2016, Oct 7 [1610.02357]   arxiv.org/abs/1610.02357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify Model for Cifar100**\n",
    "\n",
    "In the HD-CNN paper, this is represented by Shared Layers in Fig 1(b). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_layer = kr.layers.Input(shape=(128, 128, 3), dtype='float32', name='shared_layer_input')\n",
    "model = kr.applications.Xception(include_top=True, weights='imagenet', \n",
    "                                 input_tensor=in_layer, input_shape=(128, 128, 3))\n",
    "out_coarse = kr.layers.Dense(len(coarse_categories), activation='softmax')(model.layers[-2].output)\n",
    "model = kr.Model(inputs=in_layer,outputs=out_coarse)\n",
    "model.compile(optimizer=kr.optimizers.SGD(lr=0.045, momentum=0.9, decay=0),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy', 'MAE'])\n",
    "with open('./data/models/xception.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/models/xception.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Shared Layers**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This may actually be the shared layers and the component independent layers B... \n",
    "Also, we need a callback to reduce the learning rate every 2 epochs, or every step...\n",
    "And to log scalars, and for tensorboard (while we are at it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.045\n",
    "    lrate = initial_lrate * 0.94 ** np.floor(epoch/2)# Learning rate decay: decay of rate 0.94 every 2 epochs\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './logs/'+time.ctime().replace(' ', '_').replace(':', '.')\n",
    "os.mkdir(log_dir)\n",
    "callbacks=[\n",
    "    TensorBoard(log_dir=log_dir),\n",
    "    kr.callbacks.LearningRateScheduler(step_decay),\n",
    "    kr.callbacks.History()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To check in on tensorboard, copy and paste following line to cmd')\n",
    "print('tensorboard --logdir={0}'.format(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_val, y_c_val, batch_size=batch_size, initial_epoch=0, \n",
    "          validation_data=(x_val, y_c_val), epochs=stop, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_preds = model.predict(x_val, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('data/models/xception_coarse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Most Recent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/models/xception.json', 'rb') as json_file:\n",
    "    coarse_model = kr.models.model_from_json(json_file.read())\n",
    "coarse_model.load_weights('data/models/xception_coarse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.compile(\n",
    "    optimizer=kr.optimizers.SGD(lr=0.045, momentum=0.9, decay=0),\n",
    "    loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block4_sepconv1_act 26 (None, 16, 16, 256) (None, 16, 16, 256)\n",
      "block5_sepconv1_act 36 (None, 8, 8, 728) (None, 8, 8, 728)\n"
     ]
    }
   ],
   "source": [
    "for ind, layer in enumerate(coarse_model.layers):\n",
    "    if layer.name in ['block4_sepconv1_act', 'block5_sepconv1_act']:\n",
    "        print(layer.name, ind, layer.input_shape, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get output of Shared layers and cache it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_layers = kr.Model(inputs=coarse_model.input, outputs=coarse_model.layers[25].output)\n",
    "for ind, layer in enumerate(shared_layers.layers):\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_layers.compile(optimizer=kr.optimizers.SGD(lr=0.045, momentum=0.9, decay=0),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "shared_out = shared_layers.predict(x_val, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(file='./data/shared_out.npz', shared_out=shared_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(256,),\n",
       " (3, 3, 256, 1),\n",
       " (1, 1, 256, 728),\n",
       " (728,),\n",
       " (728,),\n",
       " (728,),\n",
       " (728,),\n",
       " (3, 3, 728, 1),\n",
       " (1, 1, 728, 728),\n",
       " (728,),\n",
       " (728,),\n",
       " (728,)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mem.shape for mem in coarse_model.get_weights()[43:55]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_model.layers[26].input_shape, coarse_model.layers[26].name; T = coarse_model.layers[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.get_config(), T.input.consumers()[0], T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_layers.layers[25].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr.layers.Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Fine Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Be Clear**\n",
    "\n",
    "All Layers before 26 should be considered the shared layers!\n",
    "Then I suppose we consider layers 26 - 35 (including 35) to be the coarse classifier\n",
    "And then the layers afer that would all be copied to fine classifiers... how many parameters in every fine classifier?\n",
    "\n",
    "20,847,932, which is far to many... What do you want to do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_model(course_cat):\n",
    "    callbacks=[\n",
    "        TensorBoard(log_dir=log_dir),\n",
    "        kr.callbacks.LearningRateScheduler(step_decay),\n",
    "        kr.callbacks.History()]\n",
    "    batch_size, stop = 32, 4\n",
    "    # TODO select other group\n",
    "    start, end = 5 * (course_cat - 1), 5 * course_cat\n",
    "    label = np.argmax(y_val, axis=1)\n",
    "    fine_ind = np.where(np.logical_and(label>=start, label<end))[0]\n",
    "    del label\n",
    "    fine_y = y_val[:, start:end][fine_ind]\n",
    "    fine_x = shared_out[fine_ind]\n",
    "    # The HDCNN paper has the course prediction go into the fine models...\n",
    "    _course_preds = course_preds[fine_ind, course_cat-1]\n",
    "    fine_x = np.concatenate(\n",
    "        (fine_x, \n",
    "         np.expand_dims(\n",
    "             np.apply_along_axis(\n",
    "                 lambda x: x * course_preds[fine_ind, course_cat-1], \n",
    "                 arr=np.ones(shape=(fine_x.shape[0:3])), \n",
    "                 axis=0),\n",
    "            axis=3),), \n",
    "        axis=3)\n",
    "    in_layer = Input(shape=fine_x.shape[1:], dtype='float32', name='fine_input_{0}'.format(course_cat))\n",
    "    out_fine = Dense(fine_y.shape[1:], activation='softmax')(model.layers[-2].output)\n",
    "###### Set model.layer[x]'s input tensor to be in_layer\n",
    "    model_fine = Model(inputs=in_layer, outputs=out_fine)\n",
    "    model_fine.compile(optimizer= 'adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'top_k_accuracy'])\n",
    "    model_fine.fit(fine_x, fine_y, batch_size=batch_size, initial_epoch=0, \n",
    "          validation_data=(fine_x, fine_y), epochs=stop, callbacks=callbacks)\n",
    "    return model_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_models = []\n",
    "for i in coarse_categories:\n",
    "    fine_models.append(fine_model(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Fine Classifiers on Respective Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(coarse_categories):\n",
    "    print(\"Training Fine Classifier: \", str(i))\n",
    "    \n",
    "    index= 0\n",
    "    step = 2\n",
    "    stop = 10  # Set to this only for testing purposes, change later\n",
    "    \n",
    "    # Get all training data for the coarse category\n",
    "    ind = np.where([(y_train[:,int(fine2coarse[i,j])]==1) for j in range(int(fine_categories/coarse_categories))])[1]\n",
    "    y_i = np.array([y_train[j] for j in ind])\n",
    "    x_i = np.array([x_train[j] for j in ind])\n",
    "    print(np.shape(y_i))\n",
    "    print(np.shape(x_i))\n",
    "    \n",
    "    # Get all validation data for the coarse category\n",
    "    indv = np.where([(y_val[:,int(fine2coarse[i,j])]==1) for j in range(int(fine_categories/coarse_categories))])[1]\n",
    "    y_iv = np.array([y_val[j] for j in indv])\n",
    "    x_iv = np.array([x_val[j] for j in indv])\n",
    "    print(np.shape(y_iv))\n",
    "    print(np.shape(x_iv))\n",
    "    \n",
    "    if (np.shape(x_i)[0]>0)&(np.shape(x_iv)[0]>0):\n",
    "        while index < stop:\n",
    "            fine_models['models'][i].fit(\n",
    "                x_i, y_i, batch_size=2, initial_epoch=index, epochs=index+step, validation_data=(x_iv, y_iv))\n",
    "            index += step\n",
    "            fine_models['models'][i].save_weights('data/models/model_fine_'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Probabilistic Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
